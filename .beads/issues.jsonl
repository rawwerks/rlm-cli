{"id":"rlm-cli-17z","title":"Add RAWROUTER_API_KEY detection in OpenAIClient","description":"Add auto-detection for RAWROUTER_API_KEY env var in rlm/rlm/clients/openai.py","status":"open","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:15:37.602293828-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:15:37.602293828-05:00"}
{"id":"rlm-cli-1jp","title":"Pass max_timeout, max_tokens, max_errors to child RLM in _subcall()","description":"Child RLM instances spawned via _subcall() don't inherit the parent's limit parameters.\n\n**Current behavior (rlm/rlm/core/rlm.py ~line 546-561):**\n- max_iterations: passed (same value)\n- max_depth: passed (same value)  \n- max_budget: passed (calculated as remaining budget) ✓\n- max_timeout: NOT passed\n- max_tokens: NOT passed\n- max_errors: NOT passed\n\n**Problem:**\nA child RLM can run indefinitely or consume unlimited tokens if parent has limits but child doesn't inherit them.\n\n**Fix:**\nPass these parameters to child RLM constructor in _subcall():\n- max_timeout: remaining time (parent timeout - elapsed)\n- max_tokens: same as parent (or remaining if tracked)\n- max_errors: same as parent\n\n**Source:** Feedback from RLM depth\u003e1 review","status":"closed","priority":2,"issue_type":"bug","owner":"raw@polyspectra.com","created_at":"2026-01-24T12:38:18.574937926-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T13:09:41.183158458-05:00","closed_at":"2026-01-24T13:09:41.183158458-05:00","close_reason":"Implemented in rlm/rlm/core/rlm.py - child RLM now receives max_timeout (remaining), max_tokens, max_errors, and model= override"}
{"id":"rlm-cli-1m4","title":"Reply Now: Ctrl+C returns partial answer as success","description":"Make Ctrl+C return partial answer as success instead of error.\n\n**Current behavior:**\n- Ctrl+C raises CancellationError with partial_answer attribute\n- Partial answer only mentioned in error message, not returned\n- Exit code 20 (error)\n\n**Desired behavior:**\n- Ctrl+C returns partial answer as actual output\n- Exit code 0 (success)\n- JSON metadata: {\"early_exit\": true, \"reason\": \"user_cancelled\"}\n\n**Files to modify:**\n- src/rlm_cli/rlm_adapter.py - Catch CancellationError, return partial as success\n- src/rlm_cli/output.py - Add early_exit metadata to JSON output\n\n**Verification:**\n1. Run: rlm ask . -q \"Analyze in detail\" --max-iterations 10\n2. Wait 2-3 iterations, press Ctrl+C\n3. Verify: Exit code 0, partial answer in output, early_exit: true in JSON","status":"open","priority":2,"issue_type":"feature","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:21:26.451639056-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:21:26.451639056-05:00"}
{"id":"rlm-cli-1z0","title":"Add --output-format=json-tree for hierarchical JSON","description":"Add json-tree output format that includes nested children structure.\n\nFiles:\n- src/rlm_cli/cli.py: Extend output_format choices\n- src/rlm_cli/output.py: Include tree in result payload\n\nOutput adds tree field to result:\n{\"result\": {\"response\": \"...\", \"tree\": {\"depth\": 0, \"children\": [...]}}}","status":"closed","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:45:07.122542934-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:53:35.633765851-05:00","closed_at":"2026-01-24T15:53:35.633765851-05:00","close_reason":"Implemented --output-format=json-tree with execution tree in result"}
{"id":"rlm-cli-248","title":"Add rawrouter to ClientBackend type","description":"Add 'rawrouter' to ClientBackend literal in rlm/rlm/core/types.py (line 5-15)","status":"open","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:15:35.815552348-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:15:35.815552348-05:00"}
{"id":"rlm-cli-53n","title":"Add rawrouter case to get_client()","description":"Add rawrouter routing in rlm/rlm/clients/__init__.py - set base_url to https://rawrouter.rawwerks.workers.dev/v1 and use OpenAIClient","status":"open","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:15:36.805332773-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:15:36.805332773-05:00"}
{"id":"rlm-cli-5q7","title":"Add --depth-tags flag for depth-prefixed logs","description":"Add flag to prefix all log output with [depth] tags.\n\nFiles:\n- src/rlm_cli/cli.py: Add --depth-tags flag\n- rlm/rlm/logger/verbose.py: Prefix output with depth\n\nExample output:\n[0] → Prompt: Analyze the codebase\n[1] → Prompt: Scan src directory\n[1] ← Response: Found config.py...","status":"open","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:45:08.36458038-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:45:08.36458038-05:00"}
{"id":"rlm-cli-74o","title":"RLM-CLI UX Improvements: Reply Now \u0026 Inject File","description":"Two UX features for rlm-cli:\n\n1. **Reply Now** - Ctrl+C returns partial answer as success (exit 0), plus SIGUSR1 signal handler\n2. **Inject File** - Execute user Python code in REPL between iterations via --inject-file\n\nKey insight: RLM is a REPL. Both features leverage existing infrastructure - no new mechanisms needed.\n\nWorktree: /home/raw/Documents/GitHub/rlm-cli-ux-features\nBranch: feature/ux-improvements","status":"open","priority":2,"issue_type":"epic","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:21:07.221068425-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:21:07.221068425-05:00","comments":[{"id":1,"issue_id":"rlm-cli-74o","author":"Raymond Weitekamp","text":"Child issues:\n- rlm-cli-1m4: Reply Now - Ctrl+C returns partial answer\n- rlm-cli-u74: Reply Now - SIGUSR1 signal handler (depends on rlm-cli-1m4)\n- rlm-cli-i1j: Inject File - execute user code in REPL\n\nImplementation order:\n1. rlm-cli-1m4 (Ctrl+C) - foundational, defines output format\n2. rlm-cli-u74 (SIGUSR1) - uses same output format\n3. rlm-cli-i1j (Inject File) - independent feature\n\nWorktree: /home/raw/Documents/GitHub/rlm-cli-ux-features\nBranch: feature/ux-improvements","created_at":"2026-01-24T20:22:42Z"}]}
{"id":"rlm-cli-8fk","title":"RLM Sub-Response Display Features","description":"Add CLI features to display/access recursive RLM sub-responses: tree visualization, hierarchical JSON, depth-tagged logs, and summary reports.","status":"open","priority":2,"issue_type":"epic","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:44:49.692191333-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:44:49.692191333-05:00"}
{"id":"rlm-cli-8rz","title":"Evaluate rlm-cli on benchmarks that showcase recursive decomposition","description":"## Benchmark Research Summary (from dogfooding session)\n\n### Key Insight\nRLM excels when the **STRUCTURE** of the problem requires decomposition, not just when context is long.\n\n### Tier 1 - Strongly Recommended\n\n1. **SWE-bench** (~$200-500 for full eval)\n   - Real GitHub issues requiring code fixes across repos\n   - Natural fit: use rg/tv to navigate, llm_query() per file, aggregate\n   - THE gold standard for coding agents\n\n2. **HotpotQA** (~$50-100)\n   - Multi-hop QA maps directly to recursive decomposition\n   - Cheapest, good for tuning decomposition strategies first\n   \n3. **FRAMES** (~$100-200)\n   - Multi-doc factual reasoning with provenance tracking\n   - Tests exactly what RLM is designed for\n\n4. **LongBench/InfiniteBench** (~$100-150)\n   - Proves \"infinite context via recursion\" claim\n   - Various tasks: summarization, QA, retrieval\n\n### Tier 2 - Good\n\n- RepoQA - QA over code repositories\n- BABILong - Controlled long context experiments\n- DocVQA - Multi-page document understanding\n\n### NOT Recommended\n\n- Needle-in-haystack (any chunking works, doesn't showcase RLM)\n- MMLU/TriviaQA (no decomposition needed)\n- HumanEval/MBPP (too small for recursion benefit)\n- GSM8K (chain-of-thought ≠ recursive decomposition)\n- TerminalBench (CLI focus misses the point)\n\n### Suggested Approach\n\n1. Start with HotpotQA to tune decomposition strategies\n2. Move to SWE-bench for headline results\n3. Use LongBench to prove scale claims\n\nTotal budget estimate: ~$1000-2000 for comprehensive evaluation","status":"open","priority":3,"issue_type":"feature","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:36:55.73605959-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:36:55.73605959-05:00"}
{"id":"rlm-cli-8xz","title":"Add --tree flag for real-time tree visualization","description":"Add flag to display execution as Rich Tree during runtime.\n\nFiles:\n- src/rlm_cli/cli.py: Add --tree flag\n- rlm/rlm/logger/verbose.py: Use Rich Tree widget\n\nDisplay:\n[depth=0] Analyzing codebase...\n├── [depth=1] Scanning src/ (2.3s)\n│   ├── [depth=2] Parsing config.py ✓\n│   └── [depth=2] Parsing utils.py ✓\n└── [depth=1] Scanning tests/ ✓","status":"open","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:45:09.892145852-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:45:09.892145852-05:00"}
{"id":"rlm-cli-g4h","title":"Document rawrouter in .env.example and README","description":"Add RAWROUTER_API_KEY to .env.example and document rawrouter backend usage in README.md","status":"open","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:15:39.183612908-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:15:39.183612908-05:00"}
{"id":"rlm-cli-gi6","title":"Add rawrouter auth preflight check","description":"Add rawrouter auth preflight check in src/rlm_cli/rlm_adapter.py _preflight_auth() function","status":"open","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:15:38.474747701-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:15:38.474747701-05:00"}
{"id":"rlm-cli-i1j","title":"Inject File: Execute user code in REPL between iterations","description":"Add --inject-file option to execute user Python code in REPL between iterations.\n\n**Key insight:** RLM is a REPL. The LLM writes code to manipulate variables. Users should be able to do the same - execute code to update variables, which the LLM sees on next iteration.\n\n**Implementation:**\n1. Add --inject-file CLI option (src/rlm_cli/cli.py)\n2. Before each iteration, check if inject file changed (mtime)\n3. If changed, execute file contents via environment.execute_code()\n4. No special handling - uses existing REPL infrastructure\n\n**Example:**\n```bash\n# Terminal 1\nrlm ask . -q \"Analyze auth\" --inject-file inject.py\n\n# Terminal 2 (mid-run)\necho 'focus = \"authorization\"' \u003e inject.py\n# Next iteration: LLM sees new 'focus' variable\n```\n\n**Files to modify:**\n- src/rlm_cli/cli.py - Add --inject-file option\n- rlm/rlm/core/rlm.py - Check file mtime, call execute_code() if changed\n\n**Verification:**\n1. Create inject.py with: focus = \"authentication\"\n2. Run: rlm ask . -q \"Analyze\" --inject-file inject.py\n3. Update inject.py: focus = \"authorization\"\n4. Verify: Next iteration sees updated focus value","status":"open","priority":2,"issue_type":"feature","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:21:27.521514883-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:21:27.521514883-05:00"}
{"id":"rlm-cli-nka","title":"Extract execution tree from RlmResult.raw","description":"Foundation work: Parse RlmResult.raw to extract hierarchical execution tree structure. This enables all other sub-response display features.\n\nFiles:\n- src/rlm_cli/rlm_adapter.py: Add tree extraction logic\n- src/rlm_cli/output.py: Add _build_execution_tree() helper\n\nTree structure:\n- depth, prompt_preview, response_preview, cost, duration\n- children: nested list of same structure","status":"closed","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:45:06.036725985-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:53:33.536786408-05:00","closed_at":"2026-01-24T15:53:33.536786408-05:00","close_reason":"Added iterations to RLMChatCompletion and build_execution_tree/build_execution_summary helpers"}
{"id":"rlm-cli-u74","title":"Reply Now: SIGUSR1 signal handler for graceful early exit","description":"Add SIGUSR1 signal handler for programmatic early exit.\n\n**Motivation:**\n- Ctrl+C is good for interactive use\n- SIGUSR1 is cleaner for programmatic control (scripts, orchestration)\n\n**Implementation:**\n- Add signal handler in rlm/rlm/core/rlm.py\n- Set _early_exit_requested flag when SIGUSR1 received\n- Check flag before each iteration, break gracefully with partial answer\n- Return same output as Ctrl+C (exit 0, early_exit metadata)\n\n**Files to modify:**\n- rlm/rlm/core/rlm.py - Add signal handler, check flag in main loop\n\n**Verification:**\n1. Run: rlm ask . -q \"Analyze\" --max-iterations 10 \u0026\n2. Send: kill -SIGUSR1 $!\n3. Verify: Graceful exit with partial answer","status":"open","priority":2,"issue_type":"feature","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:21:26.981867439-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:21:26.981867439-05:00","comments":[{"id":2,"issue_id":"rlm-cli-u74","author":"Raymond Weitekamp","text":"Depends on rlm-cli-1m4 being complete first (same output format/metadata structure)","created_at":"2026-01-24T20:22:49Z"}]}
{"id":"rlm-cli-vpq","title":"Add --summary flag for execution summary report","description":"Add flag to print execution summary at end.\n\nFiles:\n- src/rlm_cli/cli.py: Add --summary flag\n- rlm/rlm/logger/verbose.py: Add print_depth_summary()\n- src/rlm_cli/output.py: Add summary to JSON stats\n\nExample:\n=== RLM Execution Summary ===\nTotal depth: 3 | Nodes: 7 | Cost: $0.15\nDepth 0: 1 call ($0.05, 2.3s)\nDepth 1: 2 calls ($0.06, 4.1s)","status":"closed","priority":2,"issue_type":"task","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:45:09.155238299-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:53:36.791986229-05:00","closed_at":"2026-01-24T15:53:36.791986229-05:00","close_reason":"Implemented --summary flag with text and JSON output"}
{"id":"rlm-cli-vtx","title":"rawrouter backend integration","description":"Add rawrouter as a dedicated backend to rlm-cli. rawrouter is a personal AI gateway (OpenAI-compatible) that proxies to OpenRouter with Stripe token billing. See plan at ~/.claude/plans/frolicking-tickling-ladybug.md","status":"open","priority":2,"issue_type":"epic","owner":"raw@polyspectra.com","created_at":"2026-01-24T15:15:21.587325362-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T15:15:26.447918572-05:00"}
{"id":"rlm-cli-zb9","title":"Wire model= parameter through to child RLM in _subcall()","description":"When REPL code calls llm_query(prompt, model='haiku') and it triggers a recursive RLM (depth \u003c max_depth), the child RLM should use the specified model as its backend, not inherit the parent's default.\n\n**Current behavior:**\n- _subcall() accepts model parameter but ignores it\n- Comment in code: 'Currently uses the configured backends regardless of this parameter'\n- Child always uses parent's configured backends\n\n**Desired behavior:**\n- If model= is specified, child RLM uses that model\n- Enables cost optimization: root uses opus, subcalls use haiku\n- Enables dynamic model selection based on subtask complexity\n\n**Implementation notes:**\n- Modify _subcall() in rlm/rlm/core/rlm.py (~line 507)\n- Need to pass model to child RLM constructor or configure client\n- Consider: should this override default_backend or other_backend_client?\n\n**Source:** Feedback from RLM depth\u003e1 review","status":"closed","priority":2,"issue_type":"feature","owner":"raw@polyspectra.com","created_at":"2026-01-24T12:38:07.202494488-05:00","created_by":"Raymond Weitekamp","updated_at":"2026-01-24T13:09:41.189153884-05:00","closed_at":"2026-01-24T13:09:41.189153884-05:00","close_reason":"Implemented in rlm/rlm/core/rlm.py - child RLM now receives max_timeout (remaining), max_tokens, max_errors, and model= override"}
